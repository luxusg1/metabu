@inproceedings{feurer_efficient_2015,
	title = {Efficient and {Robust} {Automated} {Machine} {Learning}},
	volume = {28},
	url = {https://papers.nips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html},
	urldate = {2021-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
	year = {2015},
}

@inproceedings{fusi_probabilistic_2018,
	title = {Probabilistic {Matrix} {Factorization} for {Automated} {Machine} {Learning}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/b59a51a3c0bf9c5228fde841714f523a-Abstract.html},
	urldate = {2021-09-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fusi, Nicolo and Sheth, Rishit and Elibol, Melih},
	year = {2018},
}

@article{bischl_openml_2017,
	title = {{OpenML} {Benchmarking} {Suites} and the {OpenML100}},
	url = {http://arxiv.org/abs/1708.03731},
	abstract = {We advocate the use of curated, comprehensive benchmark suites of machine learning datasets, backed by standardized OpenML-based interfaces and complementary software toolkits written in Python, Java and R. Major distinguishing features of OpenML benchmark suites are (a) ease of use through standardized data formats, APIs, and existing client libraries; (b) machine-readable meta-information regarding the contents of the suite; and (c) online sharing of results, enabling large scale comparisons. As a first such suite, we propose the OpenML100, a machine learning benchmark suite of 100{\textasciitilde}classification datasets carefully curated from the thousands of datasets available on this http URL.},
	urldate = {2021-09-16},
	journal = {arXiv:1708.03731 [cs, stat]},
	author = {Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.03731
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@inproceedings{titouan_optimal_2019,
	title = {Optimal {Transport} for structured data with application on graphs},
	url = {https://proceedings.mlr.press/v97/titouan19a.html},
	abstract = {This work considers the problem of computing distances between structured objects such as undirected graphs, seen as probability distributions in a specific metric space. We consider a new transportation distance ( i.e. that minimizes a total cost of transporting probability masses) that unveils the geometric nature of the structured objects space. Unlike Wasserstein or Gromov-Wasserstein metrics that focus solely and respectively on features (by considering a metric in the feature space) or structure (by seeing structure as a metric space), our new distance exploits jointly both information, and is consequently called Fused Gromov-Wasserstein (FGW). After discussing its properties and computational aspects, we show results on a graph classification task, where our method outperforms both graph kernels and deep graph convolutional networks. Exploiting further on the metric properties of FGW, interesting geometric objects such as Fr\{é\}chet means or barycenters of graphs are illustrated and discussed in a clustering context.},
	language = {en},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Titouan, Vayer and Courty, Nicolas and Tavenard, Romain and Laetitia, Chapel and Flamary, Rémi},
	month = may,
	year = {2019},
	pages = {6275--6284},
}

@article{rivolli_meta-features_2022,
	title = {Meta-features for meta-learning},
	volume = {240},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011631},
	doi = {10.1016/j.knosys.2021.108101},
	abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. These recommendations are made based on meta-data, consisting of performance evaluations of algorithms and characterizations on prior datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in many studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents an extensive list of meta-features and characterization tools, which can be used as a guide for new practitioners. By identifying particularities and subtle issues related to the characterization measures, this survey points out possible future directions that the development of meta-features for meta-learning can assume.},
	language = {en},
	urldate = {2022-03-01},
	journal = {Knowledge-Based Systems},
	author = {Rivolli, Adriano and Garcia, Luís P. F. and Soares, Carlos and Vanschoren, Joaquin and de Carvalho, André C. P. L. F.},
	month = mar,
	year = {2022},
	keywords = {Characterization measures, Classification problems, Meta-features, Meta-learning},
	pages = {108101},
}


@book{hutter_automated_2019,
	address = {Cham},
	series = {The {Springer} {Series} on {Challenges} in {Machine} {Learning}},
	title = {Automated {Machine} {Learning}: {Methods}, {Systems}, {Challenges}},
	isbn = {9783030053178 9783030053185},
	shorttitle = {Automated {Machine} {Learning}},
	url = {http://link.springer.com/10.1007/978-3-030-05318-5},
	language = {en},
	urldate = {2021-09-08},
	publisher = {Springer International Publishing},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5},
}
